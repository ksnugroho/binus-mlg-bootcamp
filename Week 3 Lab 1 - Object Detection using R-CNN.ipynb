{"cells":[{"cell_type":"markdown","source":["# Week 3 Lab 1 - Object Detection using R-CNN"],"metadata":{"id":"awaEd4cqc4Jt"}},{"cell_type":"markdown","source":["# Problem Definition\n","**Goal:** Detect wheat heads wheat plants, including wheat datasets from around the globe.\n","\n","<img src=\"https://storage.googleapis.com/kaggle-media/competitions/UofS-Wheat/descriptionimage.png\">\n","\n","The Global Wheat Head Dataset is led by nine research institutes from seven countries: the University of Tokyo, Institut national de recherche pour l’agriculture, l’alimentation et l’environnement, Arvalis, ETHZ, University of Saskatchewan, University of Queensland, Nanjing Agricultural University, and Rothamsted Research. These institutions are joined by many in their pursuit of accurate wheat head detection, including the Global Institute for Food Security, DigitAg, Kubota, and Hiphen.\n","\n","More details on the data acquisition and processes are available at https://arxiv.org/abs/2005.02162\n"],"metadata":{"id":"k9QbHkowiFSW"}},{"cell_type":"markdown","source":["## So, what is this type of problem called?"],"metadata":{"id":"s2jPpKy4jKYv"}},{"cell_type":"markdown","source":["Our problem can be categorized as a binary classification and object localization task.\n","\n","Specifically:\n","* Classification: We are dealing with two classes—images that contain wheat and images that do not. Most images are expected to contain wheat, with only a few exceptions.\n","\n","* Localization: In addition to identifying whether an image contains wheat, it's essential to specify the exact locations of the wheat heads within the image. Simply stating the presence of wheat is insufficient; we must also perform localization to pinpoint where the wheat heads appear in the image.\n","\n","Recap: computer vision task:\n","<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*z89KwWbF59XXrsXXQCECPA.jpeg\">\n","\n","Therefore, we need a solution that can:\n","\n","* Determine whether the input image contains wheat heads.\n","* If wheat heads are present, identify their precise locations.\n","* Draw bounding boxes around the detected wheat heads.\n","* Provide a confidence score indicating how certain the algorithm is that the detected object is a wheat head."],"metadata":{"id":"2LDNi74gjOSM"}},{"cell_type":"markdown","source":["## What is Faster R-CNN\n","Faster R-CNN is a powerful technology that can address the questions we've posed. How does it achieve this?\n","\n","Before diving into the details, I will first provide a brief overview of its predecessors. This background will help clarify why Faster R-CNN has become such a popular and effective approach for object detection.\n","\n","**Background:**\n","Faster R-CNN is part of the region-based object detection methods. The development of Faster R-CNN followed this trajectory:\n","\n","* **2014:** Ross Girshick et al. introduced Regions with CNN features (R-CNN) in their groundbreaking paper.\n","* **2015:** Girshick improved upon R-CNN and proposed Fast R-CNN, making the process more efficient.\n","* **2015:** Building on Fast R-CNN, the team introduced Faster R-CNN, further enhancing speed and accuracy by incorporating a region proposal network (RPN).\n","\n","Let's briefly review each of these methods!"],"metadata":{"id":"p-asSyIGlMe-"}},{"cell_type":"markdown","source":["### R-CNN\n","\n","https://arxiv.org/abs/1311.2524\n","\n","The architecture of R-CNN:\n","\n","<img src=\"https://learnopencv.com/wp-content/uploads/2019/06/rcnn.png\">\n","\n","**Step 1:** They used an algorithm called Selective Search to generate around 2,000 region proposals. These proposals represent areas in the image that could potentially contain objects.\n","\n","**Step 2:** For each of these 2,000 bounding boxes, they applied a CNN followed by an SVM classifier to determine whether the region contained an object.\n","\n","While the accuracy of R-CNN was state-of-the-art at the time, the approach had significant speed limitations:\n","* Inference: Processing a single image took 18-20 seconds on a GPU.\n","* Training: The training process was extremely slow, taking around 84 hours and requiring a substantial amount of disk space.\n","* The method involved ad-hoc training objectives for region proposals and the SVM classifier, leading to inefficiencies."],"metadata":{"id":"fj1Omgr0lqiz"}},{"cell_type":"markdown","source":["### Fast R-CNN\n","\n","https://arxiv.org/abs/1504.08083\n","\n","The architecture of Fast R-CNN:\n","<img src=\"https://learnopencv.com/wp-content/uploads/2019/06/frcnn.png\">\n","\n","In Fast R-CNN, instead of passing 2,000 region proposals separately through multiple convolutional neural networks, the estimated region proposals (which could be fewer than 2,000) are combined into a single feature map. This unified feature map is then fed into a single neural network for processing.\n","\n","Fast R-CNN delivered a significant performance improvement by streamlining the computation, making the object detection process much faster."],"metadata":{"id":"sgg0v1y0mKFS"}},{"cell_type":"markdown","source":["### Faster R-CNN\n","\n","https://arxiv.org/abs/1506.01497\n","\n","<img src=\"https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2020/09/Fig05-2.jpg\">\n","\n","A Convolutional Neural Network (CNN) was used to generate a feature map of the image, which was simultaneously utilized for both training a Region Proposal Network (RPN) and an image classifier. This shared computation greatly improved the speed of object detection by eliminating the need for separate processing stages.\n","\n","Faster R-CNN revolutionized object detection, enabling inference to be completed in less than a second. However, the training time is longer compared to Fast R-CNN."],"metadata":{"id":"fhGHX7aJmg-v"}},{"cell_type":"markdown","source":["## Overall\n","\n","<img src=\"https://dzone.com/storage/temp/9814919-screen-shot-2018-07-23-at-114334-am.png\">"],"metadata":{"id":"jLxgsHAHnF4P"}},{"cell_type":"markdown","source":["# Download Dataset"],"metadata":{"id":"YW8FXjhwc1mx"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26431,"status":"ok","timestamp":1728701185631,"user":{"displayName":"Kuncahyo Setyo Nugroho","userId":"10958927882981450104"},"user_tz":-420},"id":"2NUwGKpznR4b","outputId":"8e84bc3a-f4ce-4fca-c8e0-9a318eadad0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1NaqtzF3GuSW_iOiwHn-I1BGKOpNUFWEa\n","From (redirected): https://drive.google.com/uc?id=1NaqtzF3GuSW_iOiwHn-I1BGKOpNUFWEa&confirm=t&uuid=583e8a85-4e6b-4445-9ce3-72d173d3f866\n","To: /content/global-wheat-detection.zip\n","100% 637M/637M [00:20<00:00, 31.0MB/s]\n"]}],"source":["# Download dataset https://drive.google.com/file/d/1NaqtzF3GuSW_iOiwHn-I1BGKOpNUFWEa/view?usp=sharing\n","!gdown 1NaqtzF3GuSW_iOiwHn-I1BGKOpNUFWEa"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_W7hkG4pZvd"},"outputs":[],"source":["# unzip dataset\n","!unzip -q global-wheat-detection.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jYWYD3Vjphk2"},"outputs":[],"source":["# See the unzip file\n","!ls"]},{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"dTS7gMgJc_32"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"gPTi604O8MDs"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import os\n","import ast\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","import torch\n","import torchvision\n","import torch.optim as optim\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torch.utils.data import DataLoader, Dataset"]},{"cell_type":"code","source":["# Set the device to GPU if available, otherwise use the CPU\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"metadata":{"id":"TrN9beAkpIUF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load & Prepare Dataset"],"metadata":{"id":"4Y4-GY2odBUT"}},{"cell_type":"code","source":["df = pd.read_csv('train.csv')\n","image_dir = 'train'"],"metadata":{"id":"Gew4damgafYH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"HrJcSbDenuXI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Augmentation"],"metadata":{"id":"83PM0-hxfwch"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BCJwapBX8ZxY"},"outputs":[],"source":["def get_train_transform():\n","    return A.Compose([\n","        A.Flip(p=0.5),              # Apply horizontal or vertical flip with a probability of 50%\n","        A.RandomRotate90(p=0.5),    # Rotate the image by 90 degrees with a probability of 50%\n","        ToTensorV2(p=1.0)           # Convert the image and its bounding boxes to PyTorch tensors\n","    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))  # Define bounding box parameters in Pascal VOC format and ensure labels are retained for transformations\n","\n","def get_valid_transform():\n","    return A.Compose([\n","        ToTensorV2(p=1.0)           # Convert the image and its bounding boxes to PyTorch tensors\n","    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))  # Use Pascal VOC format for bounding boxes and ensure labels are retained"]},{"cell_type":"markdown","source":["## Data Loader"],"metadata":{"id":"QZo1GyrAf5Ke"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CBm2qN6A8UAE"},"outputs":[],"source":["class WheatDataset(Dataset):\n","    def __init__(self, dataframe, image_dir, transforms=None):\n","        self.dataframe = dataframe      # The dataframe containing image IDs and bounding box information\n","        self.image_dir = image_dir      # Directory where images are stored\n","        self.transforms = transforms    # Any transformations to be applied (e.g., augmentations)\n","        self.image_ids = dataframe['image_id'].unique()  # Extract unique image IDs from the dataframe\n","\n","    def __len__(self):\n","        return len(self.image_ids)      # Return the total number of unique images in the dataset\n","\n","    def __getitem__(self, idx):\n","        image_id = self.image_ids[idx]  # Get the image ID at the given index\n","\n","        # Construct the full path to the image file\n","        image_path = os.path.join(self.image_dir, f\"{image_id}.jpg\")\n","\n","        # Load the image and convert it from BGR to RGB\n","        image = cv2.imread(image_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        image /= 255.0  # Normalize the image to the range [0, 1]\n","\n","        # Retrieve the bounding box data for the image\n","        records = self.dataframe[self.dataframe['image_id'] == image_id]\n","        boxes = []\n","        for i in range(len(records)):\n","            box = ast.literal_eval(records.iloc[i]['bbox'])     # Convert the bounding box string to a list\n","            boxes.append([box[0], box[1], box[0] + box[2], box[1] + box[3]])  # Convert (x, y, w, h) to (x_min, y_min, x_max, y_max)\n","\n","        boxes = np.array(boxes, dtype=np.float32)               # Convert the list of boxes to a NumPy array\n","        labels = np.ones((records.shape[0],), dtype=np.int64)   # Assign a label of 1 to all bounding boxes (since there's only one class: wheat)\n","\n","        target = {'boxes': boxes, 'labels': labels}             # Create the target dictionary with bounding boxes and labels\n","\n","        # Apply transformations if provided\n","        if self.transforms:\n","            transformed = self.transforms(image=image, bboxes=target['boxes'], labels=target['labels'].tolist())\n","            image = transformed['image']  # Apply the transformation to the image\n","            target['boxes'] = torch.tensor(transformed['bboxes'], dtype=torch.float32)  # Convert transformed bounding boxes to PyTorch tensors\n","            target['labels'] = torch.tensor(transformed['labels'], dtype=torch.int64)   # Convert transformed labels to PyTorch tensors\n","\n","        return image, target  # Return the image and the corresponding target (bounding boxes and labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wQF0VhhY8e3y"},"outputs":[],"source":["# Select a random subset of 150 unique image IDs from the dataframe\n","subset_image_ids = np.random.choice(df['image_id'].unique(), size=150, replace=False)\n","\n","# Filter the dataframe to include only the selected 100 image IDs\n","subset_df = df[df['image_id'].isin(subset_image_ids)]\n","\n","# Split the subset into training and validation sets (e.g., 80 images for training and 20 for validation)\n","train_ids, val_ids = train_test_split(subset_image_ids, test_size=0.2, random_state=42)\n","\n","# Create a dataframe by selecting rows with image IDs\n","train_df = subset_df[subset_df['image_id'].isin(train_ids)]\n","val_df = subset_df[subset_df['image_id'].isin(val_ids)]\n","\n","# Initialize the dataset using WheatDataset class and applying transformations\n","train_dataset = WheatDataset(dataframe=train_df, image_dir=image_dir, transforms=get_train_transform())\n","val_dataset = WheatDataset(dataframe=val_df, image_dir=image_dir, transforms=get_valid_transform())\n","\n","# Create DataLoader\n","train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n","val_data_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"]},{"cell_type":"markdown","source":["## Sample Image with Bounding Box"],"metadata":{"id":"eekjlfxKfqSo"}},{"cell_type":"code","source":["def visualize_sample_with_bboxes(dataset, idx):\n","    # Retrieve an image and its corresponding target (bounding boxes) from the dataset at the given index\n","    image, target = dataset[idx]\n","\n","    # Convert the image tensor (C, H, W) to a NumPy array (H, W, C) for visualization\n","    image_np = image.permute(1, 2, 0).cpu().numpy()\n","\n","    # Extract the bounding boxes from the target and convert them to a NumPy array\n","    boxes = target['boxes'].cpu().numpy()\n","\n","    # Create a matplotlib figure to display the image\n","    fig, ax = plt.subplots(1, figsize=(12, 9))\n","    ax.imshow(image_np)  # Display the image\n","\n","    # Loop through each bounding box and draw a rectangle around it\n","    for i, box in enumerate(boxes):\n","        x_min, y_min, x_max, y_max = box  # Extract the coordinates of the bounding box\n","        rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n","                                 linewidth=2, edgecolor='red', facecolor='none')  # Create a red rectangle\n","        ax.add_patch(rect)  # Add the rectangle to the image\n","\n","    plt.title(f'Image Index: {idx}')\n","    plt.show()"],"metadata":{"id":"uA5ngDCmbaoP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize_sample_with_bboxes(train_dataset, idx=0)"],"metadata":{"id":"gEb1eNobbbsf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load the Model"],"metadata":{"id":"3x-8nRQGdlyM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"V3TC2loT9C5_"},"outputs":[],"source":["def get_model(num_classes):\n","    # Load the Faster R-CNN model pre-trained on the COCO dataset\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","\n","    # Replace the model's head (box predictor) to match the number of classes\n","    # Get the number of input features for the classifier (for the head)\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","    # Replace the box predictor with a new one that has the correct number of output classes\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    return model\n","\n","# Instantiate the model for 2 classes: 1 class (wheat) + 1 background class\n","model = get_model(num_classes=2)\n","model.to(device)"]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"apNk-NmhfQZH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ItCptyvY9Gp6"},"outputs":[],"source":["def train(model, optimizer, data_loader, device, epoch):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for images, targets in tqdm(data_loader):\n","        images = [image.to(device) for image in images]\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        # Forward pass\n","        loss_dict = model(images, targets)\n","        losses = sum(loss for loss in loss_dict.values())\n","\n","        # Backward pass\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        running_loss += losses.item()\n","\n","    avg_loss = running_loss / len(data_loader)\n","    return avg_loss\n","\n","# Optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)"]},{"cell_type":"markdown","source":["<img src=\"https://storage.googleapis.com/kaggle-media/competitions/rsna/IoU.jpg\">"],"metadata":{"id":"RfUU8p6dpXij"}},{"cell_type":"code","source":["def iou(boxA, boxB):\n","    # Calculate the (x, y) coordinates of the intersection rectangle\n","    xA = max(boxA[0], boxB[0])\n","    yA = max(boxA[1], boxB[1])\n","    xB = min(boxA[2], boxB[2])\n","    yB = min(boxA[3], boxB[3])\n","\n","    # Compute the area of intersection rectangle\n","    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n","\n","    # Compute the area of both bounding boxes\n","    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n","    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n","\n","    # Compute the IoU by dividing the intersection area by the union of both areas\n","    iou = interArea / float(boxAArea + boxBArea - interArea)\n","    return iou"],"metadata":{"id":"tDikHkR4pUHM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"saKRwuiRPq75"},"outputs":[],"source":["def calculate_metrics(model, data_loader, device, iou_threshold=0.5):\n","    model.eval()  # Set model to evaluation mode\n","\n","    # Initialize accumulators for metrics\n","    all_true_positives = 0\n","    all_false_positives = 0\n","    all_false_negatives = 0\n","    total_iou = 0\n","    total_detections = 0\n","\n","    with torch.no_grad():  # No gradients needed during evaluation\n","        for images, targets in data_loader:\n","            images = [image.to(device) for image in images]  # Move images to the device\n","            outputs = model(images)  # Get model predictions\n","\n","            for i, output in enumerate(outputs):\n","                # Get predicted boxes, scores, and labels for each image\n","                pred_boxes = output['boxes'].detach().cpu().numpy()\n","                pred_scores = output['scores'].detach().cpu().numpy()\n","                pred_labels = output['labels'].detach().cpu().numpy()\n","\n","                # Filter predictions by a score threshold (e.g., keep only boxes with score > 0.5)\n","                pred_filtered = [pred_boxes[j] for j in range(len(pred_boxes)) if pred_scores[j] > 0.5]\n","\n","                # Get ground truth boxes for the current image\n","                true_boxes = targets[i]['boxes'].detach().cpu().numpy()\n","\n","                # Initialize counts for the current image\n","                true_positives = 0\n","                false_positives = 0\n","                false_negatives = 0\n","                iou_sum = 0\n","\n","                # Compare each predicted box with ground truth boxes\n","                for pred_box in pred_filtered:\n","                    max_iou = 0  # Track the highest IoU for each predicted box\n","                    for gt_box in true_boxes:\n","                        current_iou = iou(pred_box, gt_box)\n","                        max_iou = max(max_iou, current_iou)\n","\n","                    # If the highest IoU is above the threshold, count it as a true positive\n","                    if max_iou >= iou_threshold:\n","                        true_positives += 1\n","                        iou_sum += max_iou  # Add the IoU to the sum\n","                    else:\n","                        false_positives += 1\n","\n","                # Any ground truth box that was not matched with a prediction is a false negative\n","                false_negatives = len(true_boxes) - true_positives\n","\n","                # Accumulate the results across all images\n","                all_true_positives += true_positives\n","                all_false_positives += false_positives\n","                all_false_negatives += false_negatives\n","                total_iou += iou_sum\n","                total_detections += len(pred_filtered)\n","\n","    # Calculate precision, recall, F1-score, and average IoU\n","    precision = all_true_positives / (all_true_positives + all_false_positives) if (all_true_positives + all_false_positives) > 0 else 0\n","    recall = all_true_positives / (all_true_positives + all_false_negatives) if (all_true_positives + all_false_negatives) > 0 else 0\n","    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","\n","    avg_iou = total_iou / total_detections if total_detections > 0 else 0\n","\n","    return precision, recall, f1_score, avg_iou"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pO3Ong4v9YNV"},"outputs":[],"source":["num_epochs = 2\n","\n","train_losses = []\n","train_precisions = []\n","train_recalls = []\n","train_f1_scores = []\n","train_avg_ious = []\n","\n","val_precisions = []\n","val_recalls = []\n","val_f1_scores = []\n","val_avg_ious = []\n","\n","# Loop over the number of epochs\n","for epoch in range(num_epochs):\n","    # Train the model for one epoch and calculate the loss\n","    train_loss = train(model, optimizer, train_data_loader, device, epoch)\n","    train_losses.append(train_loss)  # Append the training loss\n","\n","    # Calculate metrics (Precision, Recall, F1-score, and Avg IoU) on the training data\n","    train_precision, train_recall, train_f1_score, train_avg_iou = calculate_metrics(model, train_data_loader, device)\n","    train_precisions.append(train_precision)\n","    train_recalls.append(train_recall)\n","    train_f1_scores.append(train_f1_score)\n","    train_avg_ious.append(train_avg_iou)\n","\n","    # Calculate metrics (Precision, Recall, F1-score, and Avg IoU) on the validation data\n","    val_precision, val_recall, val_f1_score, val_avg_iou = calculate_metrics(model, val_data_loader, device)\n","    val_precisions.append(val_precision)\n","    val_recalls.append(val_recall)\n","    val_f1_scores.append(val_f1_score)\n","    val_avg_ious.append(val_avg_iou)\n","\n","    # Print out the metrics for this epoch\n","    print(f\"Epoch {epoch+1}\")\n","    print(f\"Train - Loss: {train_loss}, Precision: {train_precision}, Recall: {train_recall}, F1-Score: {train_f1_score}, Average IoU: {train_avg_iou}\")\n","    print(f\"Val   - Precision: {val_precision}, Recall: {val_recall}, F1-Score: {val_f1_score}, Average IoU: {val_avg_iou}\")"]},{"cell_type":"code","source":["def plot_training_loss(train_losses):\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(train_losses, label='Training Loss', linestyle='-', color='blue', marker='o')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('Training Loss per Epoch')\n","    plt.legend()\n","    plt.show()\n","\n","plot_training_loss(train_losses)"],"metadata":{"id":"Im1t4ymyevi-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EK1AAlJE9Z5F"},"outputs":[],"source":["def plot_training_validation_metrics(train_metrics, val_metrics, metric_name, ylabel, color_train='blue', color_val='green'):\n","    plt.plot(train_metrics, label=f'Training {metric_name}', linestyle='-', color=color_train)\n","    plt.plot(val_metrics, label=f'Validation {metric_name}', linestyle='--', color=color_val)\n","    plt.xlabel('Epoch')\n","    plt.ylabel(ylabel)\n","    plt.title(f'{metric_name} per Epoch')\n","    plt.legend()\n","\n","def plot_all_metrics():\n","    plt.figure(figsize=(14, 12))\n","\n","    plt.subplot(2, 2, 1) # Plot Precision\n","    plot_training_validation_metrics(train_precisions, val_precisions, 'Precision', 'Precision', 'blue', 'green')\n","\n","    plt.subplot(2, 2, 2) # Plot Recall\n","    plot_training_validation_metrics(train_recalls, val_recalls, 'Recall', 'Recall', 'blue', 'orange')\n","\n","    plt.subplot(2, 2, 3) # Plot F1-Score\n","    plot_training_validation_metrics(train_f1_scores, val_f1_scores, 'F1-Score', 'F1-Score', 'blue', 'red')\n","\n","    plt.subplot(2, 2, 4) # Plot Average IoU\n","    plot_training_validation_metrics(train_avg_ious, val_avg_ious, 'Average IoU', 'Average IoU', 'blue', 'purple')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_all_metrics()"]},{"cell_type":"markdown","source":["# Inference"],"metadata":{"id":"suuWIknghlBR"}},{"cell_type":"code","source":["def get_test_transform():\n","    return A.Compose([\n","        ToTensorV2(p=1.0)\n","    ])\n","\n","def load_image_test(image_path, transform=None):\n","    image = cv2.imread(image_path)\n","\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","    image /= 255.0\n","\n","    if transform:\n","        transformed = transform(image=image)\n","        image = transformed['image']\n","\n","    return image"],"metadata":{"id":"VUs6jEuHgy6k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def detect_on_image(model, image_path, device):\n","    image = load_image_test(image_path, transform=get_test_transform())\n","\n","    # Add a batch dimension (for a single image) and move the image to the specified device (GPU/CPU)\n","    image = image.unsqueeze(0).to(device)\n","\n","    # Switch the model to evaluation mode\n","    model.eval()\n","\n","    # Perform inference/prediction without tracking gradients\n","    with torch.no_grad():\n","        outputs = model(image)\n","\n","    # Extract predicted bounding boxes, confidence scores, and class labels from the output\n","    pred_boxes = outputs[0]['boxes'].cpu().numpy()      # Bounding boxes for each detected object\n","    pred_scores = outputs[0]['scores'].cpu().numpy()    # Confidence scores for the detected objects\n","    pred_labels = outputs[0]['labels'].cpu().numpy()    # Class labels for each object\n","\n","    return pred_boxes, pred_scores, pred_labels"],"metadata":{"id":"sDQu19sKg0qN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_detection(image_path, boxes, scores, score_threshold=0.5):\n","    image = cv2.imread(image_path)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    fig, ax = plt.subplots(1, figsize=(12, 9))\n","    ax.imshow(image)  # Display the image on the plot\n","\n","    # Loop over each bounding box and plot it if the confidence score is above the threshold\n","    for i, box in enumerate(boxes):\n","        if scores[i] >= score_threshold:  # Filter by confidence score\n","            x_min, y_min, x_max, y_max = box  # Extract bounding box coordinates\n","\n","            # Create a rectangle for the bounding box\n","            rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n","                                     linewidth=2, edgecolor='red', facecolor='none')\n","            ax.add_patch(rect)  # Add the rectangle to the plot\n","\n","            # Add the confidence score near the top-left corner of the bounding box\n","            score = scores[i]\n","            ax.text(x_min, y_min - 5, f'{score:.2f}', color='yellow', fontsize=12,\n","                    bbox=dict(facecolor='black', alpha=0.5))  # Add text box with score\n","\n","    plt.title(f'Detection Results for {os.path.basename(image_path)}')\n","\n","    # Show the plot with bounding boxes and scores\n","    plt.show()"],"metadata":{"id":"DE8BlYbDg4Yv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ensure the trained model is loaded into memory and moved to the correct device GPU\n","model.to(device)\n","\n","# Path to the folder containing test images\n","test_image_dir = 'test'\n","\n","# Loop to perform detection on multiple images from the test folder\n","for image_name in os.listdir(test_image_dir):\n","    # Create the full path to the test image\n","    image_path = os.path.join(test_image_dir, image_name)\n","\n","    # Perform object detection on the image\n","    pred_boxes, pred_scores, _ = detect_on_image(model, image_path, device)\n","\n","    # Visualize the detection results (bounding boxes and scores)\n","    visualize_detection(image_path, pred_boxes, pred_scores, score_threshold=0.5)"],"metadata":{"id":"0MSFkDsbg6il"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMXnXdynE2WozgxiN2BJIIS"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}