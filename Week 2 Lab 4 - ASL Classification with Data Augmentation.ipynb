{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1AHrcF83Y-g"
      },
      "source": [
        "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2020/03/DLI_Feature_new.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBNeKAyF3Y-h"
      },
      "source": [
        "# 4a. Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTHY1Otu3Y-h"
      },
      "source": [
        "So far, we've selected a model architecture that vastly improves the model's performance, as it is designed to recognize important features in the images. The validation accuracy is still lagging behind the training accuracy, which is a sign of overfitting: the model is getting confused by things it has not seen before when it tests against the validation dataset.\n",
        "\n",
        "In order to teach our model to be more robust when looking at new data, we're going to programmatically increase the size and variance in our dataset. This is known as [*data augmentation*](https://link.springer.com/article/10.1186/s40537-019-0197-0), a useful technique for many deep learning applications.\n",
        "\n",
        "The increase in size gives the model more images to learn from while training. The increase in variance helps the model ignore unimportant features and select only the features that are truly important in classification, allowing it to generalize better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k01AskqI3Y-h"
      },
      "source": [
        "## 4a.1 Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCFOyxKS3Y-h"
      },
      "source": [
        "* Augment the ASL dataset\n",
        "* Use the augmented data to train an improved model\n",
        "* Save the well-trained model to disk for use in deployment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q triton"
      ],
      "metadata": {
        "id": "_2um5Dfiyr0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download utils.py\n",
        "!gdown 1SseHfqVBRrRNyETVDJkXiDsmsS2HzwkD"
      ],
      "metadata": {
        "id": "HpI0XH9rzCuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1UAbN8FBfJsQLeEx8uJcEEeS7SJf_mlab\n",
        "!gdown 1t7-vjrVdgMIxf_ATC5i7kfWHPbPhnaa8"
      ],
      "metadata": {
        "id": "fx7IXupBzdde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip asl_image.zip\n",
        "!unzip asl_data.zip"
      ],
      "metadata": {
        "id": "Xtcf4hV3zfd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocl26UO63Y-i"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms.v2 as transforms\n",
        "import torchvision.transforms.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import utils\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-FCWlRg3Y-h"
      },
      "source": [
        "## 4a.2 Preparing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjSagpmG3Y-i"
      },
      "source": [
        "As we're in a new notebook, we will load and process our data again. To do this, execute the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYhhD7yo2WEI"
      },
      "outputs": [],
      "source": [
        "IMG_HEIGHT = 28\n",
        "IMG_WIDTH = 28\n",
        "IMG_CHS = 1\n",
        "N_CLASSES = 24\n",
        "\n",
        "train_df = pd.read_csv(\"sign_mnist_train.csv\")\n",
        "valid_df = pd.read_csv(\"sign_mnist_valid.csv\")\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, base_df):\n",
        "        x_df = base_df.copy()\n",
        "        y_df = x_df.pop('label')\n",
        "        x_df = x_df.values / 255  # Normalize values from 0 to 1\n",
        "        x_df = x_df.reshape(-1, IMG_CHS, IMG_WIDTH, IMG_HEIGHT)\n",
        "        self.xs = torch.tensor(x_df).float().to(device)\n",
        "        self.ys = torch.tensor(y_df).to(device)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.xs[idx]\n",
        "        y = self.ys[idx]\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.xs)\n",
        "\n",
        "n = 32\n",
        "train_data = MyDataset(train_df)\n",
        "train_loader = DataLoader(train_data, batch_size=n, shuffle=True)\n",
        "train_N = len(train_loader.dataset)\n",
        "\n",
        "valid_data = MyDataset(valid_df)\n",
        "valid_loader = DataLoader(valid_data, batch_size=n)\n",
        "valid_N = len(valid_loader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwsfoZkE3Y-i"
      },
      "source": [
        "## 4a.3 Model Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze7Tv-Aj3Y-i"
      },
      "source": [
        "We will also need to create our model again. As we learned in the last lesson, convolutional neural networks use a repeated sequence of layers. Let's take advantage of this pattern to make our own [custom module](https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html). We can then use this module like a layer in our [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) model.\n",
        "\n",
        "To do this, we will extend the [Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class. Then we will define two methods:\n",
        "* `__init__`: defines any properties we want our module to have, including our neural network layers. We will effectively be using a model within a model.\n",
        "* `forward`: defines how we want the module to process any incoming data from the previous layer it is connected to. Since we are using a `Sequential` model, we can pass the input data into it like we are making a prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_o8Y7C91Bfl8"
      },
      "outputs": [],
      "source": [
        "class MyConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, dropout_p):\n",
        "        kernel_size = 3\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.MaxPool2d(2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XaOpCP0ya_b"
      },
      "source": [
        "Now that we've define our custom module, let's see it in action. The below model ia archecturially the same as in the previous lesson. Can you see the connection?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0A_7iJvB8Kc"
      },
      "outputs": [],
      "source": [
        "flattened_img_size = 75 * 3 * 3\n",
        "\n",
        "# Input 1 x 28 x 28\n",
        "base_model = nn.Sequential(\n",
        "    MyConvBlock(IMG_CHS, 25, 0), # 25 x 14 x 14\n",
        "    MyConvBlock(25, 50, 0.2), # 50 x 7 x 7\n",
        "    MyConvBlock(50, 75, 0),  # 75 x 3 x 3\n",
        "    # Flatten to Dense Layers\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(flattened_img_size, 512),\n",
        "    nn.Dropout(.3),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, N_CLASSES)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho65WlyRya_c"
      },
      "source": [
        "When we print the model, not only will it now show the use of our custom module, it will also show the layers within our custom module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4THc2t0HhNcv"
      },
      "outputs": [],
      "source": [
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(base_model.parameters())\n",
        "\n",
        "model = torch.compile(base_model.to(device))\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgtia6LAya_c"
      },
      "source": [
        "Custom modules are flexible, and we can define any other methods or properties we wish to have. This makes them powerful when data scientists are trying to solve complex problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjBNCzfc3Y-j"
      },
      "source": [
        "## 4a.4 Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8HdHKtM3Y-j"
      },
      "source": [
        "Before defining our training loop, it's time to set up our data augmentation.\n",
        "\n",
        "We've seen [TorchVision](https://pytorch.org/vision/stable/index.html)'s [Transforms](https://pytorch.org/vision/0.9/transforms.html) before, but in this lesson, we will further explore its data augmentation tools. First, let's get a sample image to test with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LT7NvrXhYwB"
      },
      "outputs": [],
      "source": [
        "row_0 = train_df.head(1)\n",
        "y_0 = row_0.pop('label')\n",
        "x_0 = row_0.values / 255\n",
        "x_0 = x_0.reshape(IMG_CHS, IMG_WIDTH, IMG_HEIGHT)\n",
        "x_0 = torch.tensor(x_0)\n",
        "x_0.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKFRYIpvkUEF"
      },
      "outputs": [],
      "source": [
        "image = F.to_pil_image(x_0)\n",
        "plt.imshow(image, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G9-h9q6ya_c"
      },
      "source": [
        "### 4a.4.1 [RandomResizeCrop](https://pytorch.org/vision/0.9/transforms.html#torchvision.transforms.RandomResizedCrop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3_SRvBUya_d"
      },
      "source": [
        "This transform will randomly resize the input image based on `scale`, and then [crop](https://en.wikipedia.org/wiki/Cropping_(image)) it to a size we specify. In this case, we will crop it to the original image dimensions. To do this, TorchVision needs to know the [aspect ratio](https://en.wikipedia.org/wiki/Aspect_ratio_(image)) of the image it is scaling. Since our height is the same as our width, our aspect `ratio` is 1:1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWINTqKypE5J"
      },
      "outputs": [],
      "source": [
        "trans = transforms.Compose([\n",
        "    transforms.RandomResizedCrop((IMG_WIDTH, IMG_HEIGHT), scale=(.7, 1), ratio=(1, 1)),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N2LHAKuya_d"
      },
      "source": [
        "Try running the below cell a few times. It should be different each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZugUNuJpPG2"
      },
      "outputs": [],
      "source": [
        "new_x_0 = trans(x_0)\n",
        "image = F.to_pil_image(new_x_0)\n",
        "plt.imshow(image, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VQJ1vwKp4nJ"
      },
      "outputs": [],
      "source": [
        "new_x_0.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxIRe_OGya_d"
      },
      "source": [
        "### 4a.4.2 [RandomHorizontalFlip](https://pytorch.org/vision/0.9/transforms.html#torchvision.transforms.RandomHorizontalFlip)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrmm_inJ3Y-j"
      },
      "source": [
        "We can also randomly flip our images [Horizontally](https://pytorch.org/vision/0.9/transforms.html#torchvision.transforms.RandomHorizontalFlip) or [Vertically](https://pytorch.org/vision/0.9/transforms.html#torchvision.transforms.RandomVerticalFlip). However, for these images, we will only flip them horizontally.\n",
        "\n",
        "Take a moment to think about why we would want to flip images horizontally, but not vertically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCLufCeF3Y-j"
      },
      "source": [
        "Fun fact: American Sign Language can be done with either the left or right hand being dominant. However, it is unlikely to see sign language from upside down. This kind of domain-specific reasoning can help make good decisions for your own deep learning applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2bnQMs-ya_e"
      },
      "outputs": [],
      "source": [
        "trans = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip()\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4Ay63YWya_e"
      },
      "source": [
        "Try running the below cell a few times. Does the image flip about half the time?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bkel0JKya_f"
      },
      "outputs": [],
      "source": [
        "new_x_0 = trans(x_0)\n",
        "image = F.to_pil_image(new_x_0)\n",
        "plt.imshow(image, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8BY1nWdya_f"
      },
      "source": [
        "### 4a.4.3 [RandomRotation](https://pytorch.org/vision/0.9/transforms.html#torchvision.transforms.RandomRotation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8O9verBya_f"
      },
      "source": [
        "We can also randomly rotate the image to add more variability. Just like with with other augmentation techniques, it's easy to accidentally go too far. With ASL, if we rotate too much, our `D`s might look like `G`s and visa versa. Because of this, let's limit it to `30` degrees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rf1LyY1yya_f"
      },
      "outputs": [],
      "source": [
        "trans = transforms.Compose([\n",
        "    transforms.RandomRotation(10)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e82jX89dya_f"
      },
      "source": [
        "When we run the cell block below, some black pixels may appear. The corners or our image disappear when we rotate, and for almost every pixel we lose, we gain an empty pixel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2Wu_Qj8ya_f"
      },
      "outputs": [],
      "source": [
        "new_x_0 = trans(x_0)\n",
        "image = F.to_pil_image(new_x_0)\n",
        "plt.imshow(image, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf8sM6nQya_g"
      },
      "source": [
        "### 4a.4.3 [ColorJitter](https://pytorch.org/vision/0.9/transforms.html#torchvision.transforms.ColorJitter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOEWOrJRya_g"
      },
      "source": [
        "The `ColorJitter` transform has 4 arguments:\n",
        "* [brightness](https://en.wikipedia.org/wiki/Brightness)\n",
        "* [contrast](https://en.wikipedia.org/wiki/Contrast_(vision))\n",
        "* [saturation](https://en.wikipedia.org/wiki/Colorfulness#Saturation)\n",
        "* [hue](https://en.wikipedia.org/wiki/Hue)\n",
        "\n",
        "\n",
        "The latter 2 apply to color images, so we will only use the first 2 for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbcMO-R2ya_g"
      },
      "outputs": [],
      "source": [
        "brightness = .2  # Change to be from 0 to 1\n",
        "contrast = .5  # Change to be from 0 to 1\n",
        "\n",
        "trans = transforms.Compose([\n",
        "    transforms.ColorJitter(brightness=brightness, contrast=contrast)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc2yFrjoya_g"
      },
      "source": [
        "Try running the below a few times, but also try changing either `brightness` or `contrast` to `1`. Get any intersting results?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqBIAC7wya_h"
      },
      "outputs": [],
      "source": [
        "new_x_0 = trans(x_0)\n",
        "image = F.to_pil_image(new_x_0)\n",
        "plt.imshow(image, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUc0w5sLya_h"
      },
      "source": [
        "### 4a.3.4 [Compose](https://pytorch.org/vision/0.9/transforms.html#torchvision.transforms.Compose)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUWip0dAya_h"
      },
      "source": [
        "Time to bring it all together. We can create a sequence of these random transformations with `Compose`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkXjesFKFH_b"
      },
      "outputs": [],
      "source": [
        "random_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation(5),\n",
        "    transforms.RandomResizedCrop((IMG_WIDTH, IMG_HEIGHT), scale=(.9, 1), ratio=(1, 1)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=.2, contrast=.5)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si0n452Nya_h"
      },
      "source": [
        "Let's test it out. With all the different combinations how many varations are there of this one image? Infinite?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewG_7NAgqEnf"
      },
      "outputs": [],
      "source": [
        "new_x_0 = random_transforms(x_0)\n",
        "image = F.to_pil_image(new_x_0)\n",
        "plt.imshow(image, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFjS0oUmya_h"
      },
      "source": [
        "### 4a.4 Training with Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw36tz2Kya_h"
      },
      "source": [
        "Our training is mostly the same, but there is one line of change. Before passing our images to our model, we will apply our `random_transforms`. For conveneince, we moved `get_batch_accuracy` to a [utils](./utils.py) file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcgAmvx7rI13"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    loss = 0\n",
        "    accuracy = 0\n",
        "\n",
        "    model.train()\n",
        "    for x, y in train_loader:\n",
        "        output = model(random_transforms(x))  # Updated\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss = loss_function(output, y)\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss += batch_loss.item()\n",
        "        accuracy += utils.get_batch_accuracy(output, y, train_N)\n",
        "    print('Train - Loss: {:.4f} Accuracy: {:.4f}'.format(loss, accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSjdaL-oya_i"
      },
      "source": [
        "On the other hamd, validation remains the same. There are no random transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXc6lnRAR4qZ"
      },
      "outputs": [],
      "source": [
        "def validate():\n",
        "    loss = 0\n",
        "    accuracy = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in valid_loader:\n",
        "            output = model(x)\n",
        "\n",
        "            loss += loss_function(output, y).item()\n",
        "            accuracy += utils.get_batch_accuracy(output, y, valid_N)\n",
        "    print('Valid - Loss: {:.4f} Accuracy: {:.4f}'.format(loss, accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qu9DQROya_i"
      },
      "source": [
        "Let's put data augmentation to the test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isjOJIVArTLR"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('Epoch: {}'.format(epoch))\n",
        "    train()\n",
        "    validate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0WoN84J3Y-l"
      },
      "source": [
        "## Discussion of Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EPTunxK3Y-l"
      },
      "source": [
        "You will notice that the validation accuracy is higher, and more consistent. This means that our model is no longer overfitting in the way it was; it generalizes better, making better predictions on new data.\n",
        "\n",
        "The training accuracy may be lower, and that's ok. Compared to before, the model is being exposed to a much larger variety of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npYY9cvA3Y-l"
      },
      "source": [
        "## Saving the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW_TgWkN3Y-l"
      },
      "source": [
        "Now that we have a well-trained model, we will want to deploy it to perform inference on new images.\n",
        "\n",
        "It is common, once we have a trained model that we are happy with to save it to disk. PyTorch has [multiple ways](https://pytorch.org/tutorials/beginner/saving_loading_models.html) to do this, but for now, we will use `torch.save`.\n",
        "\n",
        "PyTorch cannot save a compiled model ([see this post](https://discuss.pytorch.org/t/how-to-save-load-a-model-with-torch-compile/179739))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snAS8LalsMv4"
      },
      "outputs": [],
      "source": [
        "torch.save(base_model, 'model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfePFALr3Y-l"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fo5z3M03Y-l"
      },
      "source": [
        "In this section, you used TorchVision to augment a dataset. This resulted in a trained model with less overfitting and excellent validation image results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgDmGUB93Y-l"
      },
      "source": [
        "### Clear the Memory\n",
        "Before moving on, please execute the following cell to clear up the GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6EXCtGr3Y-l"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3I_B1M63Y-l"
      },
      "source": [
        "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2020/03/DLI_Feature_new.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}